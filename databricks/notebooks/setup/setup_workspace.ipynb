{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfaba65-aca7-4933-87f5-4095e269453e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widget for environment selection\n",
    "dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"prod\"])\n",
    "env = dbutils.widgets.get(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16977d5-9756-4732-9369-b98402884382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load config\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_root = \"/\".join(notebook_path.split(\"/\")[:5]) \n",
    "config_path = f\"/Workspace{repo_root}/config/env-config.json\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)[env]\n",
    "\n",
    "# Extract values\n",
    "storage_config = config[\"storage\"]\n",
    "databricks_config = config[\"databricks\"]\n",
    "\n",
    "catalog_name = databricks_config[\"catalog_name\"]\n",
    "storage_account = storage_config[\"account_name\"]\n",
    "storage_credential = databricks_config[\"storage_credential\"]\n",
    "\n",
    "# Build storage locations\n",
    "landing_location = f\"abfss://{storage_config['container_landing']}@{storage_account}.dfs.core.windows.net/\"\n",
    "bronze_location = f\"abfss://{storage_config['container_bronze']}@{storage_account}.dfs.core.windows.net/\"\n",
    "silver_location = f\"abfss://{storage_config['container_silver']}@{storage_account}.dfs.core.windows.net/\"\n",
    "gold_location = f\"abfss://{storage_config['container_gold']}@{storage_account}.dfs.core.windows.net/\"\n",
    "checkpoints_location = f\"abfss://{storage_config['container_checkpoints']}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "print(f\"✅ Configuration loaded:\")\n",
    "print(f\"Storage Account: {st_account_name}\")\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "\n",
    "\n",
    "# Verify storage credential exists\n",
    "print(f\"\\nChecking storage credential\")\n",
    "spark.sql(f\"DESCRIBE STORAGE CREDENTIAL {storage_credential}\").show()\n",
    "\n",
    "# Verify external location exists\n",
    "print(f\"\\nChecking external locations\")\n",
    "spark.sql(\"SHOW EXTERNAL LOCATIONS\").show()\n",
    "\n",
    "# Create catalog\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name} COMMENT 'TFL Analytics Pipeline catalog for {env} environment'\")\n",
    "print(f\"\\n✅ Catalog '{catalog_name}' created\")\n",
    "\n",
    "# Use the catalog\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "\n",
    "# Create schemas\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS landing LOCATION '{landing_location}'\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS bronze MANAGED LOCATION '{bronze_location}'\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS silver MANAGED LOCATION '{silver_location}'\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS gold LOCATION '{gold_location}'\")\n",
    "print(f\"✅ Schemas created: landing, bronze, silver, gold\")\n",
    "\n",
    "# Create EXTERNAL volume pointing to landing container\n",
    "spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {catalog_name}.landing.tfl_raw LOCATION '{landing_location}'\")\n",
    "print(f\"✅ External volume created: {catalog_name}.landing.tfl_raw\")\n",
    "print(f\"   Location: {landing_location}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SETUP COMPLETE - {env.upper()} ENVIRONMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").show(truncate=False)\n",
    "print(f\"\\n✅ Ready for data ingestion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe676ce-2885-486d-b547-3b00ba407a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "setup_workspace",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "aa8d447f-d727-4bab-83ad-d3b0e83acebc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
