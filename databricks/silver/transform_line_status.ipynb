{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c69726-f9fd-4e02-a7bf-6d6b8155586b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d30954b-8af3-43db-9e21-948832ce4b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../common/transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e766231c-2674-447d-a90a-e1ee7059152f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../common/data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08e4994-6ae5-42d2-b3d7-f9fedb862538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "target_table = \"lines_sv\"\n",
    "silver_table_path = get_storage_path(\"silver\", target_table)\n",
    "\n",
    "# Ensure silver table exists\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema_silver}.{target_table}(\n",
    "    line_id STRING,\n",
    "    service_type STRING,\n",
    "    severity_code BIGINT,\n",
    "    severity_description STRING,\n",
    "    disruption_category STRING,\n",
    "    disruption_description STRING,\n",
    "    disruption_from_date TIMESTAMP,\n",
    "    disruption_to_date TIMESTAMP,\n",
    "    is_service_disrupted BOOLEAN,\n",
    "    event_timestamp TIMESTAMP\n",
    ") \n",
    "LOCATION '{silver_table_path}'\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Transform and load silver table\n",
    "source_table = get_table_name(schema_bronze, \"lines_bz\")\n",
    "\n",
    "df_transformed = spark.read.table(source_table).select(\n",
    "    col(\"id\").alias(\"line_id\"),\n",
    "    get(col(\"serviceTypes\"), 0)[\"name\"].alias(\"service_type\"),\n",
    "    get(col(\"lineStatuses\"), 0)[\"statusSeverity\"].alias(\"severity_code\"),\n",
    "    get(col(\"lineStatuses\"), 0)[\"statusSeverityDescription\"].alias(\n",
    "        \"severity_description\"\n",
    "    ),\n",
    "    get(col(\"lineStatuses\"), 0)[\"disruption\"][\"category\"].alias(\"disruption_category\"),\n",
    "    get(col(\"lineStatuses\"), 0)[\"disruption\"][\"description\"].alias(\n",
    "        \"disruption_description\"\n",
    "    ),\n",
    "    to_timestamp(\n",
    "        get(get(col(\"lineStatuses\"), 0)[\"validityPeriods\"], 0)[\"fromDate\"]\n",
    "    ).alias(\"disruption_from_date\"),\n",
    "    to_timestamp(\n",
    "        get(get(col(\"lineStatuses\"), 0)[\"validityPeriods\"], 0)[\"toDate\"]\n",
    "    ).alias(\"disruption_to_date\"),\n",
    "    when(\n",
    "        get(col(\"lineStatuses\"), 0)[\"disruption\"][\"description\"].isNotNull()\n",
    "        & (get(col(\"lineStatuses\"), 0)[\"disruption\"][\"description\"] != \"\"),\n",
    "        lit(True),\n",
    "    )\n",
    "    .otherwise(lit(False))\n",
    "    .alias(\"is_service_disrupted\"),\n",
    "    col(\"created\").cast(\"timestamp\").alias(\"event_timestamp\"),\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "df_cleaned = trim_strings(df_transformed)\n",
    "df_deduped = df_cleaned.dropDuplicates([\"line_id\", \"event_timestamp\"])\n",
    "\n",
    "df_quality = add_quality_flag(\n",
    "    df_deduped, not_null_columns=[\"line_id\", \"event_timestamp\"]\n",
    ")\n",
    "df_silver = add_transformation_metadata(df_quality)\n",
    "\n",
    "# Write to silver table\n",
    "query = (\n",
    "    df_silver.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{catalog}.{schema_silver}.{target_table}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_line_status",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
