{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c69726-f9fd-4e02-a7bf-6d6b8155586b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32743200-4d35-478d-85cc-bee2dcd72bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../common/transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a5b62b-7978-4cbc-9ca4-26c5fef43516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../common/data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08e4994-6ae5-42d2-b3d7-f9fedb862538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "target_table = \"arrivals_sv\"\n",
    "silver_table_path = get_storage_path(\"silver\", target_table)\n",
    "\n",
    "# Ensure silver table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema_silver}.{target_table}(\n",
    "    arrival_id STRING,\n",
    "    operation_type BIGINT,\n",
    "    vehicle_id STRING,\n",
    "    naptan_id STRING,\n",
    "    station_name STRING,\n",
    "    line_id STRING,\n",
    "    platform_name STRING,\n",
    "    direction STRING,\n",
    "    bearing BIGINT,\n",
    "    trip_id BIGINT,\n",
    "    base_version BIGINT,\n",
    "    destination_naptan_id STRING,\n",
    "    destination_name STRING,\n",
    "    event_timestamp TIMESTAMP,\n",
    "    time_to_station BIGINT,\n",
    "    current_location STRING,\n",
    "    towards STRING,\n",
    "    expected_arrival TIMESTAMP,\n",
    "    time_to_live TIMESTAMP,\n",
    "    ingestion_timestamp TIMESTAMP,\n",
    "    dq_flag STRING,\n",
    "    transformation_timestamp TIMESTAMP\n",
    ")\n",
    "LOCATION '{silver_table_path}'\n",
    "\"\"\")\n",
    "\n",
    "# Transform and load silver table\n",
    "source_table = get_table_name(schema_bronze, \"arrivals_bz\")\n",
    "silver_checkpoint = get_storage_path(\"checkpoints\", \"silver\")\n",
    "\n",
    "df_transformed = (\n",
    "    spark.readStream\n",
    "        .option(\"ignoreDeletes\", True)\n",
    "        .table(source_table)\n",
    "        .select(\n",
    "            col(\"id\").alias(\"arrival_id\"),\n",
    "            col(\"operationType\").alias(\"operation_type\"),\n",
    "            col(\"vehicleId\").alias(\"vehicle_id\"),\n",
    "            col(\"naptanId\").alias(\"naptan_id\"),\n",
    "            col(\"stationName\").alias(\"station_name\"),\n",
    "            col(\"lineId\").alias(\"line_id\"),\n",
    "            col(\"platformName\").alias(\"platform_name\"),\n",
    "            col(\"direction\"),\n",
    "            col(\"bearing\"),\n",
    "            col(\"tripId\").alias(\"trip_id\"),\n",
    "            col(\"baseVersion\").alias(\"base_version\"),\n",
    "            col(\"destinationNaptanId\").alias(\"destination_naptan_id\"),\n",
    "            col(\"destinationName\").alias(\"destination_name\"),\n",
    "            col(\"timestamp\").cast(\"timestamp\").alias(\"event_timestamp\"),\n",
    "            col(\"timeToStation\").alias(\"time_to_station\"),\n",
    "            col(\"currentLocation\").alias(\"current_location\"),\n",
    "            col(\"towards\"),\n",
    "            col(\"expectedArrival\").cast(\"timestamp\").alias(\"expected_arrival\"),\n",
    "            col(\"timeToLive\").cast(\"timestamp\").alias(\"time_to_live\"),\n",
    "            col(\"_ingest_time\").alias(\"ingestion_timestamp\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "df_cleaned = trim_strings(df_transformed)\n",
    "df_deduped = (\n",
    "    df_cleaned\n",
    "    .withWatermark(\"event_timestamp\", \"30 seconds\")\n",
    "    .dropDuplicates([\"arrival_id\", \"event_timestamp\"])\n",
    ")\n",
    "\n",
    "df_quality = add_quality_flag(df_deduped, not_null_columns=[\"arrival_id\",\"naptan_id\",\"line_id\",\"event_timestamp\"])\n",
    "df_silver = add_transformation_metadata(df_quality)\n",
    "\n",
    "# Write data to silver table\n",
    "query = (\n",
    "    df_silver.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{silver_checkpoint}/{target_table}\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{catalog}.{schema_silver}.{target_table}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_bus_arrivals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
